sqrt(2.8^2+0.4^2)
sqrt(2.3^2+1.9^2)
sqrt(1.2^2+2.3^2)
sqrt(4+1.3^2)
(0.7+0.333333)^2+(0.33333+0.1)^2+(0.9-0.333333)^2
(0.7+0.333333)^2+(0.33333+0.1)^2+(0.9-0.333333)^2 - 0.18
install.packages("arules")
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
data(Epub)
class(Epub)
summary(Epub)
length(EPub)
length(Epub)
inspect(Epub[1:5])
Epub[1:5]
inspect(Epub[1:15])
length(Epub)
inspect(Epub[15729])
image(Epub[1:1000])
dim(Epub)
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
inpect(rules)
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
inspect(rules)
help(apriori)
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
source('~/Dropbox/cornell/ORIE4740/lectApr9.R')
1+1
unif
unif()
unif()?
dunif(x,min = 0, max = 6, log = FALSE)
nubmer.of.dice.rolls = 100
dice.rolls = runif(nubmer.of.dice.rolls,1,6)
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
dice.rolls
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
vector.sum.of.rolls.greater.than.m
cumDiceSum
number.of.rolls.until.m
cumDiceSum > m
vector.sum.of.rolls.greater.than.m[cumDiceSum > m]
cumDiceSum > m
cumDiceSum < m
cumDiceSum[cumDiceSum < m]
cumDiceSum[sum(cumDiceSum < m)]
cumDiceSum[sum(cumDiceSum < m)+1]
source('~/Desktop/DataIncubatorChallenge.R')
vector.sum.of.rolls.greater.than.m
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
dist(vector.number.of.rolls.until.m)
clear
clc
mean(vector.number.of.rolls.until.m)
mean(vector.sum.of.rolls.greater.than.m)
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m)
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
n = 100000
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
n = 1000
m = 100
vector.number.of.rolls.until.m = vector()
vector.sum.of.rolls.greater.than.m = vector()
for (i in 1:n)
{
nubmer.of.dice.rolls = 100
dice.rolls = sample(1:6,20,replace=TRUE)
cumDiceSum = cumsum(dice.rolls)
number.of.rolls.until.m = sum(cumDiceSum<m)+1
vector.number.of.rolls.until.m[i] = number.of.rolls.until.m
vector.sum.of.rolls.greater.than.m[i] =  cumDiceSum[sum(cumDiceSum < m)+1]
}
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
rm(list = ls())
n = 1000
m = 100
vector.number.of.rolls.until.m = vector()
vector.sum.of.rolls.greater.than.m = vector()
for (i in 1:n)
{
nubmer.of.dice.rolls = 100
mean(c(1,2,3,4,5,6))
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
rm(list = ls())
n = 10
m = 100
vector.number.of.rolls.until.m = vector()
vector.sum.of.rolls.greater.than.m = vector()
for (i in 1:n)
{
nubmer.of.dice.rolls = 100
dice.rolls = sample(1:6,ceiling(m/3.5),replace=TRUE)
cumDiceSum = cumsum(dice.rolls)
dice.rolls = sample(1:6,ceiling(m/3.5),replace=TRUE)
dice.rolls = sample(1:6,ceiling(m/3.5),replace=TRUE)
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.sum.of.rolls.greater.than.m) - m
sd(vector.sum.of.rolls.greater.than.m) - m
mean(vector.number.of.rolls.until.m)
source('~/Desktop/DataIncubatorChallenge.R')
mean(vector.number.of.rolls.until.m)
sd(vector.sum.of.rolls.greater.than.m)
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
print("the mean of the sum minus M when M = 20 is %d",mean(vector.sum.of.rolls.greater.than.m) - m )
source('~/Desktop/DataIncubatorChallenge.R')
foo = mean(vector.sum.of.rolls.greater.than.m) - m
paste("the mean of the sum minus M when M = 20 is %d",foo )
shor
head(vector.number.of.rolls.until.m)
head(vector.sum.of.rolls.greater.than.m)
source('~/Desktop/DataIncubatorChallenge.R')
foo = mean(vector.sum.of.rolls.greater.than.m) - m
paste("the mean of the sum minus M when M = 20 is",foo )
paste("the mean number of rolls when M = 20 is", mean(vector.number.of.rolls.until.m))
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
paste("the mean of the sum minus M when M = 20 is",mean(vector.sum.of.rolls.greater.than.m) - m )
paste("the standard deviation of the sum minus M when M = 20 is",sd(vector.sum.of.rolls.greater.than.m) - m)
paste("the mean number of rolls when M = 20 is", mean(vector.number.of.rolls.until.m))
paste("the standard deviation of the number of rolls M = 20 is", sd(vector.number.of.rolls.until.m))
mean(vector.sum.of.rolls.greater.than.m) - m
rm(list = ls())
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
source('~/Desktop/DataIncubatorChallenge.R')
a = [10.22,10.13,10.18,10.12,10.22,10.17,10.13,10.11,10.14,10.19]
a = [10.22,10.13,10.18,10.12,10.22,10.17,10.13,10.11,10.14,10.19]
a = (10.22,10.13,10.18,10.12,10.22,10.17,10.13,10.11,10.14,10.19)
a = (1,2,3)
a = c(10.22,10.13,10.18,10.12,10.22,10.17,10.13,10.11,10.14,10.19)
a
avg(a)
mean(a)
var(a)
va
var(a)
b = c(4.63,4.7,4.62,4.63,4.69,4.64,4.72,4.63)
mean(b)
source('~/Dropbox/DataIncubatorChallenge/TaxiScript.R')
a = c(10.13,10.10,10.14,10.12,10.12,10.19,10.14,10.12,10.11,10.14,10.13,10.13,10.13)
a
var(a)
*4
b = c(4.64,4.62,4.69,4.65,4.72,4.66)
var(b)
mean
mean(b)
require(graphics)
rm(list = ls())
setwd("~/Dropbox/cornell/ORIE4740/labsHw/lab1")
oliveData = read.table("olive-train.dat")
oliveTestData = read.table("olive-test.dat")
colnames(oliveData)= c("region","area","palmiticAcidPct","palmitoleicAcidPct","stearicAcidPct","oleicAcidPct","linoleicAcidPct","linolenicAcidPct","arachidicAcidPct","eicosenoicAcidPct")
colnames(oliveTestData)= c("region","area","palmiticAcidPct","palmitoleicAcidPct","stearicAcidPct","oleicAcidPct","linoleicAcidPct","linolenicAcidPct","arachidicAcidPct","eicosenoicAcidPct")
oliveData = oliveData[ oliveData$region != 1,]
dim(oliveData)
#print(oliveData[1:10,])
oliveData$region = as.factor(oliveData$region)
oliveData$area = as.factor(oliveData$area)
oliveTestData$region = as.factor(oliveTestData$region)
oliveTestData$area = as.factor(oliveTestData$area)
fit <- lm(oliveData$oleicAcidPct~oliveData$linoleicAcidPct, data=faithful)
par(mfrow=c(1,1))
#plot(oliveData$oleicAcidPct,oliveData$linoleicAcidPct, main="linoleic vs oleic",col = c("red","green")[ oliveData$region ],xlim = c(7250,7750),ylim = c(800,1200))
plot(oliveData$oleicAcidPct,oliveData$linoleicAcidPct, main="linoleic vs oleic",col = c("red","green")[ oliveData$region ])
#lines( oliveData$oleicAcidPct, oliveData$linoleicAcidPct, fitted(fit), col="blue")
##This Part scatterplots out all of the different combinations of variables to show you the potential clusters to be found when looking
par(mfrow=c(2,5))
for(i in 3:10)
{
#par(mfrow = c(6,7))
for (j in 3:10)
{
if (i != j)
{
xName = names(oliveData)[i]
yName = names(oliveData)[j]
plot(oliveData[[i]],oliveData[[j]],xlab = xName,ylab = yName,col = c("red","green")[ oliveData$region ])
}
}
}
head(oliveData)
head(oliveData[[1]])
head(oliveData[1,)
head(oliveData[1,])
head(oliveData[,1])
par(mfrow = c(1,1))
plot(oliveData$linoleicAcidPct,oliveData$linolenicAcidPct,xlab = "linoleicAcidPct",ylab = "linolenicAcidPct",col = c("red","green")[ oliveData$region ])
par(mfrow = c(1,1))
plot(oliveData$linoleicAcidPct,oliveData$linolenicAcidPct,xlab = "linoleicAcidPct",ylab = "linolenicAcidPct",col = c("red","green")[ oliveData$region ])
abline(308,-0.28)
lines(c(850,1100),c(70,0))
oliveRegionPredict <- function(testDataFrame){
setwd("~/Dropbox/cornell/ORIE4740/labsHw/lab2")
rm(list = ls())
accidentsData = read.table("accidentsData.csv",header = TRUE, sep = ",")
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
jointDist = t(t(jointCounts)/rowSums(jointCounts))
print(jointDist)
setwd("~/Dropbox/cornell/ORIE4740/labsHw/lab2")
rm(list = ls())
accidentsData = read.table("accidentsData.csv",header = TRUE, sep = ",")
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
jointDist = t(t(jointCounts)/rowSums(jointCounts))
print(jointDist)
head(accidentsData)
head(accidentsData[[20]])
names(accidentsData)
names(accidentsData[20])
accidentsData = read.table("accidentsData.csv",header = TRUE, sep = ",")
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
jointDist = t(t(jointCounts)/rowSums(jointCounts))
print(jointDist)
print(joingCounts)
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
jointCounts
jointDist = t(t(jointCounts)/rowSums(jointCounts))
print(jointDist)
print(jointDist)
jointCounts
help(t)
(jointCounts)/rowSums(jointCounts)
rowSums(jointCounts)
jointCounts
rowSums(jointCounts)
(jointCounts)/rowSums(jointCounts)
t(jointCounts)/rowSums(jointCounts)
print(jointDist)
accidentsData = accidentsData[,1:20]
trainData = accidentsData[-(1:1000),]
nb.train <- function(D = NULL){
#======================================================================
# Function: nb.train
# from nbc-r, available on Google Code
# GPL-2 license
#                                                              by TSYo
#                                              last updated:2005.11.10
#----------------------------------------------------------------------
# Description:
#     training a naive Bayes model from a given data matrix.
#----------------------------------------------------------------------
# Input arguments:
#     D
#         A data matrix containing the training data.  Each row of D
#         contains one training example, with the class at the last
#         column.
#----------------------------------------------------------------------
# Return objects:
source('~/Dropbox/cornell/ORIE4740/labsHw/lab2/lab2Script.R')
source('~/Dropbox/cornell/ORIE4740/labsHw/lab2/lab2Script.R', echo=TRUE)
head(accidentPrediction)
jointDist = t(t(jointCounts)/rowSums(jointCounts)) #transpose tha shit
accidentsData = read.table("accidentsData.csv",header = TRUE, sep = ",")
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
print(jointDist)
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
jointDist = t(t(jointCounts)/rowSums(jointCounts)) #transpose tha shit
print(jointDist)
jointCounts = table(accidentsData$INJURY_CRASH, accidentsData$INT_HWY)
print(jointCounts)
print(jointDist)
accidentsData = accidentsData[,1:20]
accidentPrediction
accidentsData
trainData = accidentsData[-(1:1000),]
trainData = accidentsData[-(1:1000),]
accidentsData = accidentsData[,1:20]
nb.train <- function(D = NULL){
#======================================================================
# Function: nb.train
#                                              last updated:2005.11.10
# from nbc-r, available on Google Code
# GPL-2 license
#                                                              by TSYo
# Description:
#----------------------------------------------------------------------
#     training a naive Bayes model from a given data matrix.
#----------------------------------------------------------------------
# Input arguments:
#     D
#         A data matrix containing the training data.  Each row of D
#         contains one training example, with the class at the last
#         column.
#----------------------------------------------------------------------
# Return objects:
#     class.dist
#         A vector contains the relative frequencies of the class labels.
#     attr.dist
#         A list of matrices which contain the conditional distribution
#         of the corresponding attribute given the class label.
#----------------------------------------------------------------------
# Examples:
#     nbWSBC <- nb.train(wsbc.dat)    # Train a naive Bayes classifier
#     nbWSBC$class.dist               # Show the prob. of each class label
# Checking arguments
#======================================================================
stop("Please specify the matrix containing training data.")
if(missing(D))
# Probability calculation
#
#----------------------------------------------------------------------
dimD = dim(D)
nRow = dimD[1]                    # size of training sample
nAttr = dimD[2] -1                # number of attributes
cRow  = dimD[2]                   # row number of the class label
adist = NULL
# Calculate Pr(C)
cdist = table(D[,cRow])/nRow
# Calculate Pr(A_j|C)=Pr(A_j,C)/Pr(C)
for (j in 1:nAttr){
tmpAdist = table(D[,cRow],D[,j])/nRow      # Pr(A_i,C)
for(i in 1:dim(cdist)){
tmpAdist[i,] = tmpAdist[i,]/cdist[i]   # Pr(A_i,C)/Pr(C)
}
#----------------------------------------------------------------------
adist = c(adist,list(tmpAdist))
}
# End of probability calculation
return(list(class.dist=cdist,attr.dist=adist))
# Return results
}
#======================================================================
nb.predict <- function(D = NULL,nb = NULL){
# Function: nb.predict
# from nbc-r, available on Google Code
#                                                              by TSYo
#                                              last updated:2005.10.22
#----------------------------------------------------------------------
# Usage:
#     nb.predict(D, nb)
#----------------------------------------------------------------------
# Description:
#     predicting the given data based on the given naive Bayes model.
#----------------------------------------------------------------------
# Input arguments:
#     D
#         A data matrix containing the data to be predicted.  The last
#     nb
#         column contains the true class labels to be predicted.
#----------------------------------------------------------------------
#         A naive Bayes model used to make predictions.
#     class.pred
# Return objects:
#         A vector contains the predicted class labels.
#         The confusion matrix, i.e. a cross-table of the true class
#     conf.mat
#         labels and the predicted class labels.
#     error.rate
#         An estimate of the error rate of nb based on the predictive
#         accuracy of nb on D.
# Examples:
#----------------------------------------------------------------------
#======================================================================
#
# Checking arguments
if(missing(D))
stop("Please specify the matrix containing testing data.")
if(missing(nb))
#
# Parameters
stop("Please specify the naive Bayes model to be used for prediction.")
#----------------------------------------------------------------------
#  From testing data
dimD = dim(D)
dataRow = dimD[1]                      # size of the testing data
dataCol = dimD[2]                      # number of attributes + 1 (for class labels)
#  From the naive Bayes model
nbPrClass = nb$class.dist              # Prob. of each class label provided by the model
nbPrAttr = nb$attr.dist                # Prob. of each attribute with its possible values
nClass = dim(nbPrClass)                # Number of class labels
nAttr  = length(nbPrAttr)              # Number of attributes
# Matrix contains the prob. of each class for each row
prClass = matrix(1, ncol = nClass, nrow = dataRow)
#  For classification
#
#----------------------------------------------------------------------
# Probability calculation
#     (1) Pr(C=i) can be looked up in nbPrClass[i]
#   Pr(C=i|Aj) = Pr(C=i)*Prod(j){Pr(Aj|C=i)}/Sum(i){Pr(C=i)*prod{Pr(Aj|C=i)}}
#     (2) Pr(Aj|C=i) can be looked up in nbPrAttr[[#Attr]][i,ValOfAttr]
#     (3) The denominator is a constant for each class, can be ignored
#
for (j in 1:dataRow){              # For each records
for (i in 1:nClass){                   # Value of class
for (k in 1:nAttr){            # For each attribute
# the following has been altered by Dawn Woodard on 09/07/08
l = as.character( D[j,k] ) # Value of the attribute
thisAttrClasses = colnames(nbPrAttr[[k]])
# Prod(j){Pr(Aj|C=i)}
if( sum( l == thisAttrClasses ) == 0 ){
# predictor value not in training set
prClass[j,i] = 0
prClass[j,i] = nbPrClass[i] * prClass[j,i]
}
prClass[j,i] = prClass[j,i]*nbPrAttr[[k]][i, which(l==thisAttrClasses) ]
} else {
# Pr(C=i)*Prod(j){Pr(Aj|C=i)}
}
}
}
#----------------------------------------------------------------------
# End of probability calculation
# Predicting the class label:
#   Select the class label with maximum probability.  If more than one
#   class have max prob, select the first one.
classNames = names( nbPrClass )
predClass = rep(0,dataRow)
# the following has been altered by Dawn Woodard on 09/07/08
for (i in 1:dataRow){
classInd = which(prClass[i,] == max(prClass[i,]))[1]
predClass[i] = classNames[ classInd ]
}
#----------------------------------------------------------------------
# Creating confusion martix:
#
trueClass = as.character(D[,dataCol])
confuMatrix = table(trueClass,predClass)
#----------------------------------------------------------------------
# Calculating error rate:
#
err = sum(abs(predClass != trueClass),na.rm=T)/dataRow
#----------------------------------------------------------------------
# Return results
return(list(class.pred=predClass ,conf.mat=confuMatrix ,error.rate=err ))
}
fooModel = nb.train(trainData)
accidentPrediction = nb.predict(D = accidentsData[1:1000,] , nb = fooModel)
